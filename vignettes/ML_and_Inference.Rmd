---
title: "Machine learning and statistical inference on persistence diagrams with TDAInference"
author: "Shael Brown"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{inference}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
references:
- id: Robinson_Turner
  title: 'Hypothesis testing for topological data analysis'
  author:
  - family: Robinson
    given: Andrew
  - family: Turner
    given: Katharine
  container-title: Journal of Applied and Computational Topology
  volume: 1
  URL: 'https://doi.org/10.1007/s41468-017-0008-7'
  DOI: 10.1007/s41468-017-0008-7
  page: 241
  type: article-journal
  issued:
    year: 2017
- id: persistence_fisher
  title: 'Persistence fisher kernel: a riemannian manifold kernel for persistence diagrams'
  author:
  - family: Le
    given: Tam
  - family: Yamada
    given: Makoto
  container-title: Neurips
  type: article-journal
  issued:
    year: 2018
- id: distance_calc
  title: 'Geometry Helps to Compare Persistence Diagrams'
  author:
  - family: Kerber
    given: Michael
  - family: Morozov
    given: Dmitriy
  - family: Nigmetov
    given: Arnur
  container-title: ACM Journal of Experimental Algorithmics
  volume: 22
  URL: 'https://doi.org/10.1145/3064175'
  DOI: 10.1145/3064175
  page: 1
  type: article-journal
  issued:
    year: 2017
- id: dependencies
  title: 'Statistical Inference for Persistent Homology applied to fMRI'
  author:
  - family: Abdallah
    given: Hassan
  - family: Regalski
    given: Adam
  - family: Behzad Kang
    given: Mohammad
  - family: Berishaj
    given: Maria
  - family: Nnadi
    given: Nkechi
  - family: Chowdury
    given: Asadur
  - family: Diwadkar
    given: Vaibhav
  - family: Salch
    given: Andrew
  container-title: Github
  URL: 'https://github.com/hassan-abdallah/Statistical_Inference_PH_fMRI/blob/main/Abdallah_et_al_Statistical_Inference_PH_fMRI.pdf'
  type: article-journal
  issued:
    year: 2021
- id: kernel_test
  title: 'A Kernel Statistical Test of Independence'
  author:
  - family: Gretton
    given: Arthur
  - family: Fukumizu
    given: Kenji
  - family: Hui Teo
    given: Choon
  - family: Song
    given: Le
  - family: Scholkopf
    given: Bernhard
  - family: Smola
    given: Alexander
  container-title: NEURIPS
  type: article-journal
  issued:
    year: 2007
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Statistical inference of persistence diagrams

In this package functionality is provided to carry out machine learning methods and to make statistical inference on groups of persistence diagrams. For machine learning, currently we can project a group of persistence diagrams into low dimensions using either metric multidimensional scaling (MDS) or kernel principal components analysis (kPCA), cluster a group of diagrams with kernel k-means (kkmeans) or perform kernel support vector machine (kSVM) classification or regression where the inputs are the persistence diagrams. For inference, we have groups of persistence diagrams and can test if there is either a difference between them using a permutation test, or if the (two) groups are independent or not. The permutation test is detailed in @Robinson_Turner, where we compute an analogue to within group variance by calculating distances between each same-group pair of persistence diagrams, and then repeat this calculation many times for permuted group labels. The p-value for the test is computed as a smoothed ratio of the number of permutations which resulted in a within-group variance which was lower than the originally-calculated test statistic to the total number of permutations. The independence test is parametric, based on the paper @kernel_test, and with a small p-value we can find evidence that the two groups are not independent. Now, we'll provide more details as to how the computations are actually carried out.

One of the central objects of interest in topological data analysis (TDA) is called a persistence diagram. A persistence diagram for a point cloud data set is a topological descriptor which keeps track of various topological features of the data set. A persistence diagram is represented as a set of 2D points above the diagonal (i.e. where $y \geq x$), for each topological dimension. One way we can compute distances between persistence diagrams in the same dimension is by finding optimal matching of their 2D points, either using a $p$-Minkowski distance leading to the $p$-wasserstein distance or using the infinity-norm distance leading to the ''bottleneck distance'' @distance_calc. The inifinity-norm distance between two 2D points is just the maximum absolute distance between either coordinate: $d_{\infty} = \max\{|x_{1}-x_{2}|,|y_{1}-y_{2}|\}$. A matching of two persistence diagrams $D_{1},D_{2}$ in the same dimension $dim$ is $\phi:D_{1} \rightarrow D_{2}$ where each point $p_{1}$ in $D_{1}$ is "matched" to a point $\phi(p_{1})$ which is either a point in $D_{2}$ or is the projection (nearest point) of $p_{1}$ onto the diagonal, and every point in $D_{2}$ is matched to either a point in $D_{1}$ or to its projection on the diagonal. To compute the $p$-wasserstein distance we find the optimal matching by minimizing the loss function

$[\sum_{p_{1}}d_{\infty}(p_{1},\phi(p_{1}))^p]^{1/p}$, and the bottleneck distance is found by minimizing the loss function $\mbox{max}\{d_{\infty}(p_{1},\phi(p_{1}))\}$. These optimization problems can be solved exactly by the Hungarian algorithm in the clue package.

Another distance metric between persistence diagrams is called the Fisher information metric, $d_{FIM}$ (details can be found in @persistence_fisher). The idea is to represent the two persistence diagrams as probability density functions, with a Gaussian point mass centered at each pointin the diagram (including the diagonal projections of the opposite diagram), and computing the Fisher information metric between the two distributions. Specifically, we calculate the following two vectors: $\rho_1 = [\sum_{u \in D_1 \cup \Delta D_2}N(x;u,\sigma^2)|u \in D_1 \cup \Delta D_2 \cup D_2 \cup \Delta D_1]$ and $\rho_2 = [\sum_{u \in D_2 \cup \Delta D_1}N(x;u,\sigma^2)|u \in D_1 \cup \Delta D_2 \cup D_2 \cup \Delta D_1]$. Then we noramlize them by their sums, $\overline{\rho_1} = \frac{\rho_1}{\sum \rho_1}$ and $\overline{\rho_2} = \frac{\rho_2}{\sum \rho_2}$. Finally, we compute $<\sqrt{\rho_1},\sqrt{\rho_2}>$, i.e. the dot product of the elementwise square root of both of the noramlized vectors.

One use of distances between diagrams is embedding a group of diagrams into a low-dimensional space using metric MDS. Metric MDS takes as input a $n$ by $n$ distance matrix and returns an embedding of $n$ points into $d$-dimensional Euclidean space such that the Euclidean distance between the embedded points are as close as possible to the original distance matrix. 

Now that we have defined distances between persistence diagrams we can discuss how to make inferences about groups of persistence diagrams. For the basic idea we will consider two groups of persistence diagrams, $G_{1}$ and $G_{2}$, containing $n_{1}$ and $n_{2}$ persistence diagrams respectively. Each persistence diagram comes from a dataset, which could be viewed as a sample from a true "underlying" manifold (geometric object). Therefore, we might want to test if the groups $G_{1}$ and $G_{2}$ are samples of persistence diagrams coming from the same manifold, $H_{0}$, or if the underlying manifolds are different $H_{A}$. We start by computing a test-statistic, $L$, which is essentially the sum of within-group variances:

$$L = \frac{1}{n_{1}(n_{1}-1)}\sum_{D_{1} \neq D_{2} \in G_{1}}d(D_{1},D_{2})^{q} + \frac{1}{n_{2}(n_{2}-1)}\sum_{D_{1} \neq D_{2} \in G_{2}}d(D_{1},D_{2})^{q}$$

Here, $q$ is some finite exponent which is at least 1, and $d$ is the desired distance of persistence diagrams. $L$ is the test statistic for our hypothesis test.

Since relatively little is known about the distribution of persistence diagrams a permutation test procedure allows us to determine a suitable null distribution for making inferences. We will shuffle the group labels of the persistence diagrams (effectively moving them between the two groups) $N$ times and recompute the test statistic each time to obtain a list of "permutation values" $\{L_{1},\dots,L_{N}\}$. The idea is that if the groups are truly different, i.e. coming from different manifold objects, then there should be relatively small within group variances compared to when we shuffle the group labels. Therefore, let $NZ$ be the number of permutations $i$ such that $L_{i} < L$. Then the p-value of the test is $\frac{NZ+1}{N+1}$.

The extension to multiple groups, noted at the end of the paper @Robinson_Turner, is extremely straightforward and is implemented in this package. The null hypothesis is that all the groups are sampled from the same geometric object, and the alternative hypothesis is that some of the groups are sampled from different geometric objects. The test statistic is still the sum of the (essentially) within-group variances across all the groups, the permutations shuffle the persistence diagrams between all the groups, and the p-value is computed in the same way.

There is one additional functionality that this package provides, and it is an idea that was discussed in @dependencies. One of the assumptions of the inference procedure listed in @Robinson_Turner is that the persistence diagrams are all independent, however this is not always reasonable to assume in practice. For example, suppose we had point cloud data for several subjects in a pharmaceutical study both before and after a drug intervention. We might want to know if the geometric structure of the point clouds changed after the drug intervention, so our two groups of persistence diagrams would be the before intervention and after invention groups. Since there is a pairing between some of the persistence diagrams (the diagrams belonging to the same patient) these diagrams will not be independent of each other. From this perspective, we will only allow permutations which keep the two (or more in the case of many diagram groups) persistence diagrams of each patient (i.e. dependency) separated. So after each permutation each group still contains one persistence diagram per patient. Not only is this more ideal theoretically, but it also offers a practical advantage. If there was truly a difference between the groups then if there are subject differences the permuted-within-group variances would be larger when each group contains one persistence diagram from each subject, enhancing the test power.

In order to use kernel methods for machine learning of persistence diagrams, this package implements the kernel described in @persistence_fisher. The idea is quite simple, let $\sigma,t > 0$. Then the persistence fisher kernel is defined as $\rho(D_1,D_2) = \mbox{exp}(-t*d_{FIM}(D_1,D_2))$, and the Gram matrix of a list of persistence diagrams $D_1,D_2,\dots,D_n$ is the matrix which stores the kernel values: $G[i,j] = \rho(D_i,D_j)$. We can use this kernel to perform several machine learning tasks with persistence diagrams: we can embed a group of persistence diagrams into a low-dimensional space using kPCA by computing eigenvectors of the Gram matrix, we can cluster a group of persistence diagrams using kkmeans, and we can train classification and regression models where the training set is a list of persistence diagrams using kSVM's. TDAML also provides prediction methods for kkmeans, kPCA and kSVM.

To demonstrate the utility of the TDAInference package, we will go through several examples. Firstly, we will give some examples to show that the various distances between persistence diagrams are being computed correctly. Secondly, we will benchmark the distance calculations to show that they are faster than those provided in the TDA package. Thirdly, we will carry out various permutation tests with multiple groups either sampled from the same or different objects to show that the permutation test returns expected results.


```{r setup}
# if(interactive()){
#  devtools::load_all()
# }
library("TDAML")
```

## Case study: `Using the diagram_distance function`

First we will construct a pair of small diagrams for which we can easily calculate by hand what their distance should be (according to the various types of distance metrics available in this package), and then we will do the same with diagrams generated from the TDA package. Let $D_1 = \{(2,3)\}$ be the diagram with a single point $(2,3)$, and let $D_2 = \{(2,3.1),(5,6)\}$ have two points. We will construct these diagrams as data frames, as if they were the output of the diagram_to_df function, and plot them:

```{r}
# D1
D1 = data.frame(dimension = 0,birth = 2,death = 3)
plot(x = D1$birth,y = D1$death,xlab = "Birth",ylab = "Death",main = bquote("" ~ D[1]),xlim = c(0,6),ylim = c(0,6),pch = 16)
abline(a = 0,b = 1)

# D2
D2 = data.frame(dimension = 0,birth = c(2,5),death = c(3.1,6))
plot(x = D2$birth,y = D2$death,xlab = "Birth",ylab = "Death",main = bquote("" ~ D[2]),xlim = c(0,6),ylim = c(0,6),pch = 16)
abline(a = 0,b = 1)

```

Now, let's compute the distance between $D_1$ and $D_2$ using the various distance metrics, and confirm that  it's clear what the distance value should be. Note that for all metrics the optimal matching is the same: $(2,3)$ is matched with $(2,3.1)$ and $(5,6)$ is matched with its diagonal projection, $(5.5,5.5)$. Therefore, using the 2-wasserstein metric (like in the package TDA) the distance should be $\sqrt{0.1^2+0.5^2} \approx 0.509902$:

```{r}
diagram_distance(D1 = D1,D2 = D2,p = 2,distance = "wasserstein",dim = 0)
```

The same distance but with $p = 3$ should be $(0.1^3+0.5^3)^{1/3} \approx 0.5013298$:

```{r}
diagram_distance(D1 = D1,D2 = D2,p = 3,distance = "wasserstein",dim = 0)
```

The persistence Fisher distance between $D_1$ and $D_2$ using sigma = 1 should be calculated as follows. $\rho_1 = \frac{1}{2\pi}(e^0+e^{-0.2525}+e^{-9.25},e^{-0.2525}+e^0+e^{-8.7025},e^{-9.25}+e^{-8.7025}+e^0,e^{-0.005}+e^{-0.3025}+e^{-9.005},e^{-9}+e^{-8.9525}+e^{-0.25},e^{-0.25}+e^{-0.0025}+e^{-9})$ and $\rho_2 = \frac{1}{2\pi}(e^{-0.005}+e^{-9}+e^{-0.25},e^{-0.3025}+e^{-8.9525}+e^{-0.0025},e^{-9.005}+e^{-0.25}+e^{-9},e^{0}+e^{-8.705}+e^{-0.305},e^{-8.705}+e^{0}+e^{-9.25},e^{-0.305} + e^{-9.25} + e^{0})$. Then the distance metric should equal $<\sqrt{\overline{\rho_1}},\sqrt{\overline{\rho_2}}> \approx 0.9984164$. When we use the diagram distance function with distance set to fisher we see that we get the same result:

```{r}
diagram_distance(D1 = D1,D2 = D2,distance = "fisher",dim = 0,sigma = 1)
```

Finally, we can compute the standard bottleneck distance between the diagrams, which is just the maximum bottleneck distance between any pair of matched points, 0.5:

```{r}
diagram_distance(D1 = D1,D2 = D2,p = Inf,distance = "wasserstein",dim = 0)
```

We include more tests of correctness for the diagram_distance function in the test folder, should the reader be interested in further validation.

<!-- OLD -->

<!-- $D_3 = \{(1,1.1),(3,3.1)\}$. We can visualize the three diagrams below: -->

<!-- ```{r} -->
<!-- D3 = data.frame(x = c(1,3),y = c(1.1,3.1)) -->
<!-- ``` -->

<!-- We can do similar calculations for each of the other pairs $(D_1,D_3)$ and $(D_2,D_3)$. For $(D_1,D_3)$ the optimal matchine is each point with its diagonal projection: $(2,3)$ with $(2.5,2.5)$, $(1.1.1)$ with $(1.05,1.05)$ and $(3,3.1)$ with $(3.05,3.05)$. It is easy to verify that the distance should be $\sqrt{0.05^2+0.05^2+0.5^2} \approx 0.5049752$ for the 2-wasserstein metric, $\sqrt{0.05^2+0.05^2+0.05^2+0.05^2+0.5^2+0.5^2} \approx 0.7141428$ for the $p = 2$ Turner metric, and $0.5$ for the Infinity norm metric. We see that those calculations were carried out correctly by the digram_distance function as well: -->

<!-- It is easy to verify that the distances between $D_2$ and $D_3$ are computed properly, but it covers similar cases to previous distances so we will not write out the verification in this document. -->

## Case study: `Benchmarking the diagram_distance function`

Now we will benchmark the runtime of the distance calculations on pairs of different-sized random persistence diagrams compared to calculations using the package TDA. We will first simulate random diagrams of various sizes to compare runtimes, with $n$ points ($n \in \{100,200,300,\dots,10000\}$) with 10 iterations at each value of $n$. Each diagram has birth values of 0.1 multiplied by a random sample of a chi squared distribution with 10 degrees of freedom, and the death was the birth plus 0.1 times a sample from a chi squared distribution with 1 degree of freedom. The code took much too long to generate with this vignette, but can be viewed in the .Rmd file for this vignette. The runtimes were computed on a Windows 10 64-bit machine, an Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz 3.60 GHz processor with 8 cores and 64GB of RAM. The benchmarking plot can be seen below:

```{r,echo = F,warning=F,out.width="80%",out.height="90%"}
# simulate random barcodes with sizes 100,200,...,1000
# library(TDA)
# runtimes_random <- data.frame(n_row = numeric(),package = character(),time_in_sec = numeric())
# for(n_row in seq(100,1000,100)){
# 
#   for(iteration in 1:10)
#   {
# 
#     # simulate pair of diagrams, points mainly close to the diagonal
#     diagram1 = data.frame(dimension = rep(0,n_row),birth = 0.1*rchisq(n = n_row,df = 10))
#     diagram1$death = diagram1$birth + 0.1*rchisq(n = n_row,df = 1)
#     diagram2 = data.frame(dimension = rep(0,n_row),birth = 0.1*rchisq(n = n_row,df = 10))
#     diagram2$death = diagram2$birth + 0.1*rchisq(n = n_row,df = 1)
# 
#     # benchmark time, in seconds, for both package's distance calculations (2-wasserstein distance in TDAInference)
#     start_time_TDAInference = Sys.time()
#     d_TDAInference = diagram_distance(D1 = diagram1,D2 = diagram2,dim = 0,p = 2,distance = "wasserstein")
#     end_time_TDAInference = Sys.time()
#     time_diff_TDAInference = as.numeric(end_time_TDAInference - start_time_TDAInference,units = "secs")
#     diagram1 = list(diagram = diagram1)
#     diagram2 = list(diagram = diagram2)
#     diagram1$diagram = as.matrix(diagram1$diagram)
#     diagram2$diagram = as.matrix(diagram2$diagram)
#     class(diagram1$diagram) = "diagram"
#     class(diagram2$diagram) = "diagram"
#     start_time_TDA = Sys.time()
#     d_TDA = TDA::wasserstein(Diag1 = diagram1$diagram,Diag2 = diagram2$diagram,dimension = 0,p = 2)
#     end_time_TDA = Sys.time()
#     time_diff_TDA = as.numeric(end_time_TDA - start_time_TDA,units = "secs")
# 
#     runtimes_random = rbind(runtimes_random,data.frame(n_row = n_row,package = "TDAInference",time_in_sec = time_diff_TDAInference))
#     runtimes_random = rbind(runtimes_random,data.frame(n_row = n_row,package = "TDA",time_in_sec = time_diff_TDA))
# 
#   }
# 
# }
# 
# # compute means and sd's at each value of rows for both packages
# summary_table = data.frame(n_row = numeric(),mean = numeric(),sd = numeric(),package = character())
# for(n_row in seq(100,1000,100))
# {
#   
#   for(p in c("TDAInference","TDA"))
#   {
#     
#     summary_table = rbind(summary_table,data.frame(n_row = n_row,mean = mean(runtimes_random[which(runtimes_random$n_row == n_row & runtimes_random$package == p),3]),sd = sd(runtimes_random[which(runtimes_random$n_row == n_row & runtimes_random$package == p),3]),package = p))
#     
#   }
#   
# }

# since this code takes a very long time to run, we will hard code the table that resulted from the above section^
summary_table = data.frame(n_row = rep(seq(100,1000,100),each = 2),mean = c(0.087,0.137,0.278,1.880,0.859,8.742,1.782,28.908,3.928,63.544,6.648,141.373,11.158,252.689,18.409,427.515,28.220,725.520,39.675,1024.007),sd = c(0.021,0.010,0.018,0.170,0.100,0.630,0.161,1.849,0.245,3.927,0.455,12.202,0.806,14.315,1.350,37.318,2.675,63.963,2.983,102.328),package = rep(c("TDAInference","TDA"),10))

# plot table
plot(summary_table$n_row[summary_table$package=="TDA"], summary_table$mean[summary_table$package=="TDA"], type="b",
     xlim=range(summary_table$n_row), ylim=range(0,summary_table$mean+1.96*summary_table$sd),xlab = "Points in diagram",ylab = "Mean execution time (sec)",main = "Benchmarking diagram_distance function")
lines(summary_table$n_row[summary_table$package=="TDAInference"], summary_table$mean[summary_table$package=="TDAInference"], col=2, type="b")
legend(x = 200,y = 800,legend = c("TDAInference","TDA"),col = c("red","black"),lty = c(1,1),cex = 0.8)
arrows(summary_table$n_row, summary_table$mean-1.96*summary_table$sd/sqrt(10), summary_table$n_row, summary_table$mean+1.96*summary_table$sd/sqrt(10), length=0.05, angle=90, code=3)

```

The TDAInference diagram\_distance function gives a significant speedup compared to the TDA wasserstein function (with similar results expected for the bottleneck distance function since the two use the same underlying engine). As the diagrams get larger the speedup appears to increase, with almost a 26x speedup for diagrams with 1000 points, however this ratio looks like it would plateau around 30x above 1000 points, or at least it would grow slowly. 

Now, we will repeat the same experiment but instead computing diagrams from shapes provided in the TDA package (a uniformly sampled Torus and sphere) with the same number of data points as in the previous example, $n \in \{100,200,\dots,1000\}$ with 10 repetitions per size. The homology calculations had a maximum dimension of two, and the maximum scale for each was 2 (for the Torus) and 1 (for the sphere). The Torus had tube radius 1 and cross-radius 2. Again, to avoid a long build for this vignette the results were computed on the same computer as listed above, and can be generated using the code in the .Rmd file. The plot is as follows:

```{r,echo = F,warning=F}
# generate persistence diagrams from circles, Tori and spheres with  100,200,...,1000 data points.
# library(TDA)
# runtimes_shape <- data.frame(n_row = numeric(),package = character(),time_in_sec = numeric())
# for(n_row in seq(100,1000,100)){
# 
#   for(iteration in 1:10)
#   {
# 
#     # simulate pair of diagrams from the desired shapes
#     diagram_torus = ripsDiag(X = TDA::torusUnif(n = n_row,a = 1,c = 2),maxdimension = 2,maxscale = 2)
#     diagram_sphere = ripsDiag(X = TDA::sphereUnif(n = n_row,d = 2,r = 1),maxdimension = 2,maxscale = 1)
#     
#     # compute their wasserstein distances in all dimensions and benchmark
#     start_time_TDAInference = Sys.time()
#     diagram_distance(D1 = diagram_torus,D2 = diagram_sphere,dim = 0,p = 2,distance = "wasserstein")
#     diagram_distance(D1 = diagram_torus,D2 = diagram_sphere,dim = 1,p = 2,distance = "wasserstein")
#     diagram_distance(D1 = diagram_torus,D2 = diagram_sphere,dim = 2,p = 2,distance = "wasserstein")
#     end_time_TDAInference = Sys.time()
#     time_diff_TDAInference = as.numeric(end_time_TDAInference - start_time_TDAInference,units = "secs")
#     
#     start_time_TDA = Sys.time()
#     TDA::wasserstein(Diag1 = diagram_torus$diagram,Diag2 = diagram_sphere$diagram,dimension = 0,p = 2)
#     TDA::wasserstein(Diag1 = diagram_torus$diagram,Diag2 = diagram_sphere$diagram,dimension = 1,p = 2)
#     TDA::wasserstein(Diag1 = diagram_torus$diagram,Diag2 = diagram_sphere$diagram,dimension = 2,p = 2)
#     end_time_TDA = Sys.time()
#     time_diff_TDA = as.numeric(end_time_TDA - start_time_TDA,units = "secs")
# 
#     runtimes_shape = rbind(runtimes_shape,data.frame(n_row = n_row,package = "TDAInference",time_in_sec = time_diff_TDAInference))
#     runtimes_shape = rbind(runtimes_shape,data.frame(n_row = n_row,package = "TDA",time_in_sec = time_diff_TDA))
# 
#   }
#   print(paste0("Done ",n_row," rows"))
# 
# }
# 
# # compute means and sd's at each value of rows for both packages
# summary_table = data.frame(n_row = numeric(),mean = numeric(),sd = numeric(),package = character())
# for(n_row in seq(100,1000,100))
# {
# 
#   for(p in c("TDAInference","TDA"))
#   {
# 
#     summary_table = rbind(summary_table,data.frame(n_row = n_row,mean = mean(runtimes_shape[which(runtimes_shape$n_row == n_row & runtimes_shape$package == p),3]),sd = sd(runtimes_shape[which(runtimes_shape$n_row == n_row & runtimes_shape$package == p),3]),package = p))
# 
#   }
# 
# }

# here is the output that I got on my local computer
summary_table = data.frame(n_row = rep(seq(100,1000,100),each = 2),mean = c(0.133,0.220,0.398,3.430,0.953,17.054,1.955,53.896,3.760,142.524,6.604,294.552,10.466,601.802,16.169,1130.289,23.458,2091.953,35.098,3518.517),sd = c(0.022,0.016,0.035,0.186,0.094,1.1889,0.130,4.501,0.211,5.924,0.772,11.244,0.623,9.064,0.989,43.385,1.989,88.432,2.747,172.684),package = rep(c("TDAInference","TDA"),10))

# plot table
plot(summary_table$n_row[summary_table$package=="TDA"], summary_table$mean[summary_table$package=="TDA"], type="b",
     xlim=range(summary_table$n_row), ylim=range(0,summary_table$mean+1.96*summary_table$sd),xlab = "Points in shapes",ylab = "Mean execution time (sec)",main = "Benchmarking diagram_distance function on Torus and Sphere")
lines(summary_table$n_row[summary_table$package=="TDAInference"], summary_table$mean[summary_table$package=="TDAInference"], col=2, type="b")
legend(x = 200,y = 2000,legend = c("TDAInference","TDA"),col = c("red","black"),lty = c(1,1),cex = 0.8)
arrows(summary_table$n_row, summary_table$mean-1.96*summary_table$sd/sqrt(10), summary_table$n_row, summary_table$mean+1.96*summary_table$sd/sqrt(10), length=0.05, angle=90, code=3)

```

Once again, the runtime of the diagram\_distance function is significantly smaller. The ratio of runtimes grew at each subsequent number of tested data points in the shapes, with a 100x speed up with 1000 data points. These two simulations point towards TDAInference a fast distance calculation between persistence diagrams, and that performing many such computations, like in the case of a permutation test of persistence diagrams, could become much more feasible.

## Case study: `Using the permutation_test function`

The permutation\_test function can be used to make statistical inferences about any number of groups of persistence diagrams computed from the TDA package. To see this functionality, we will create three groups of diagrams with unequal numbers of elements:

```{r,echo = T}

g1 <- lapply(X = 1:3,FUN = function(X){

  diag <- TDA::ripsDiag(TDA::circleUnif(n = 100,r = 1),
                                   maxscale = 1,
                                   maxdimension = 1)
  df <- diagram_to_df(d = diag)
  return(df)

})

g2 <- lapply(X = 1:3,FUN = function(X){

  diag <- TDA::ripsDiag(TDA::torusUnif(n = 100,c = 2,a = 1),
                                   maxscale = 1,
                                   maxdimension = 1)
  df <- diagram_to_df(d = diag)
  return(df)

})

g3 <- lapply(X = 1:2,FUN = function(X){

  diag <- TDA::ripsDiag(TDA::sphereUnif(n = 100,r = 1,d = 2),
                                   maxscale = 1,
                                   maxdimension = 1)
  df <- diagram_to_df(d = diag)
  return(df)

})

```

Since the groups are not of the same size, we cannot set the paired argument to be True. The permutation test could be carried out, with mostly default parameters, by

```{r,warning=FALSE}
perm_test = permutation_test(g1,g2,g3,verbose = TRUE)
```

We can access the various components of the test easily. The p-values in each dimension can be viewed with

```{r}
perm_test$p_value
```

The test statistic can be viewed with

```{r}
perm_test$test_statistic
```

The permutation values can be accessed in a list, with one element for each desired dimension.

```{r}
perm_test$permvals$`0`
```

Other parameter values that can be viewed are the dimensions in which the test was carried out and the runtime of the test. Lastly, we will carry out the same calculation, but with pairing. To that end all groups must have the same number of elements:

```{r}
g3[3] <- g3[1]
perm_test_paired = permutation_test(g1,g2,g3,verbose = T,paired = T)
```

This paired parameter should be set to True when there is a natural pairing between the diagrams in the groups. For example, if each group represents the persistence diagram of (in this case) three subject's fMRI data, and the three groups represent three different fMRI tasks, then a pairing between the groups exists. Setting the pairing parameter to True in this case can reduce the variance in permutation values, thereby reducing the likelihood of spurious significant test results.


## References


