---
title: "Machine learning and statistical inference of persistence diagrams with TDAML"
author: "Shael Brown"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{inference}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: REFERENCES.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

Topological data analysis is a new area of data science which can find unique, non-linear global structure in whole datasets. The main tool of topological data analysis is called persistent homology, which produces a "shape descriptor" of its input dataset called a "persistence diagram". Two main R packages exist for computing persistence diagrams; however, the more flexible of the two, called TDA, stores persistence diagrams in a strange data structure. Methods exist for computing distances and similarities (kernels) between pairs of persistence diagrams, but only distance calculations are available in R packages. Moreover, several papers have used distance and kernel computations in order to perform machine learning or inferences tasks with groups of persistence diagrams. To date no publicly available software in either python or R provides the functionality for these types of analyses. In order to make the power of topological data analysis available to data and machine learning practitioners, a software package must exist which can (1) convert the output of persistent homology into a commonly-used data type for data analyses (a data frame), (2) implement fast versions of both distance and kernel calculations, and (3) provide scalable methods for machine learning and inference using persistence diagrams. It is with these goals that the package TDAML was created.

# Software Review

Topological data analysis has gained popularity over the past two decades since the original paper on persistent homology was published (@PHoriginal), and three main R packages have been created for topological data analysis calculations. The R package TDA (@R-TDA) is a wrapper for a number of C++ persistent homology packages, including Dionysus (@Dionysus), GUDHI (@GUDHI) and PHAT (@PHAT), and provides distance calculation functionality. The R package TDAstats (@R-TDAstats) is a wrapper of the C++ persistent homology engine Ripser (@Ripser), and provides a method for inference of two groups of persistence diagrams (as in @Robinson_Turner). However, the distance calculation currently implemented, as of the date of writing, is not a standard distance metric for persistence diagrams. It is worthwhile to note that there are several python packages dedicated to topological data analysis calculations, including scikit-TDA (@scikittda2019) (computing persistence diagrams, and distances/kernels between them), and giotto-tda (@tauzin2020giottotda) (computing persistence diagrams and using persistence diagrams to analyze time series data).

There are a number of current shortcomings of available topological data analysis software. Firstly, in both python and R there is no package which allows for machine learning and inference of persistence diagrams, a limitation which greatly constrains the types of analyses that can be carried out. In R this is partially because there is no package for kernel calculations of persistence diagrams, but the very slow computation of distances also inhibits the practicality of distance-based inference procedures. Additionally, in R, the output of persistent homology calculations from the package TDA is a list with an element called "diagram" of class "diagram". The package TDA provides some nice additional functionality compared to TDAstats, including the ability to return the location, in the dataset, of each topological feature found in the persistence diagram. Therefore, the unnatural data type returned by persistent homology calculations in the package TDA may also be limiting the development of TDA applications. 

# Package TDAML

The package TDAML aims to solve the three goals outlined in the introductory paragraph. TDAML is built to use as input the output of persistent homology calculations from the package TDA. Firstly, the function `diagram_to_df` allows the conversion of the output of TDA persistent homology calculations to a data frame. Secondly, the functions `diagram_distance` and `diagram_kernel` allow for fast distance and kernel calculations respectively, and their counterparts `distance_matrix` and `gram_matrix` compute in parallel, for scalability, the (cross) distance and (cross) Gram matrices respectively. Thirdly, these distance and kernel calculations are used to perform machine learning and inference on persistence diagrams - methods include 

1. dimension reduction with metric multidimensional scaling (MDS) and kernel princpal components analysis (kpca),
2. clustering with kernel k-means,
3. regression and classification with kernel support-vector machines (ksvm), and
4. inference with distance and kernel calculations looking for group differences and group independence respectively

The kernel machine learning methods implemented are wrappers of the flexible R package for kernel calculations kernlab (@R-kernlab), with some additional processing steps specific to persistence diagrams. In the subsequent sections we will describe these applications in more detail.

# Background: persistent homology

The main tool of topological data analysis is called persistent homology (see (@PHoriginal) for the introductory paper, and (@ComputingPH) for further computational details). Persistent homology has been applied in a variety of areas, including (but not limited to) economics (largely for the application of time series, for example see (@PHeconomics)), neuroscience (see (@review_of_PH_for_fMRI) for a number of functional MRI applications), etc. 

Persistent homology starts with data points and a distance function. It assumes that these points were sampled from some kind of continuous shape. This shape has certain features that exist at various scales, but sampling induces noise in these features. Persistent homology aims to describe certain mathematical features of this underlying shape, by forming approximations to the shape at various distance scales. The mathematical features which are tracked are clusters (connected components), loops (ellipses), voids (spheres), etc, and the "significance" of each feature is calculated. What's really interesting about these particular mathematical features is that they can tell us where our data is not, which is extremely important information which other data analysis methods can't provide.

```{r,echo = F,out.width="80%",out.height="90%"}
circ <- TDA::circleUnif(n = 50,r = 1)
par(mfrow = c(1,4),mar = c(1,1,1,1))
plot(x = circ[,1],y = circ[,2],xlab = "Approximation 1: individual data points",ylab = "")
plot(x = circ[,1],y = circ[,2],xlab = "Approximation 2: no loop",ylab = "")
for(i in 1:(nrow(circ)-1))
{
  for(j in (i+1):nrow(circ))
  {
    if(sqrt((circ[i,1]-circ[j,1])^2+(circ[i,2] - circ[j,2])^2) <= 0.2)
    {
      lines(c(circ[i,1],circ[j,1]),c(circ[i,2],circ[j,2]))
    }
  }
}
plot(x = circ[,1],y = circ[,2],xlab = "Approximation 2: loop",ylab = "")
for(i in 1:(nrow(circ)-1))
{
  for(j in (i+1):nrow(circ))
  {
    if(sqrt((circ[i,1]-circ[j,1])^2+(circ[i,2] - circ[j,2])^2) <= 0.4)
    {
      lines(c(circ[i,1],circ[j,1]),c(circ[i,2],circ[j,2]))
    }
  }
}
plot(x = circ[,1],y = circ[,2],xlab = "Approximation 3: no loop",ylab = "")
for(i in 1:(nrow(circ)-1))
{
  for(j in (i+1):nrow(circ))
  {
    if(sqrt((circ[i,1]-circ[j,1])^2+(circ[i,2] - circ[j,2])^2) <= 2)
    {
      lines(c(circ[i,1],circ[j,1]),c(circ[i,2],circ[j,2]))
    }
  }
}
par(mfrow = c(1,1))
```

The persistent homology algorithm proceeds in the following manner: first, if the input is a dataset and distance metric then the distance matrix, storing the distance metric value of each pair of points in the dataset, is computed. Next, a parameter $\epsilon \geq 0$ is grown starting at 0, and at each $\epsilon$ value we compute a skeletal approximation of the dataset structure $C_{\epsilon}$ called a "simplicial complex" (see (@PHoriginal) or (@ComputingPH) for more details) by connecting all pairs of points whose distance is at most $\epsilon$. To encode higher-dimensional structure in these approximations, we also add a triangle between any triple of points which are all connected, a tetrahedron between any quadruple of points which are all connected, etc. Note that this process of forming a sequence of skeletal approximations is called a filtration, and other methods exist for forming the approximations (the one described here is the most commonly used, called the Rips-Vietoris complex).

At any given $\epsilon$ value, some topological features will exist in $C_{\epsilon}$. As $\epsilon$ grows, the $C_{\epsilon}$'s will contain eachother, i.e. if $\epsilon_{1} < \epsilon_{2}$ then every edge (triangle, tetrahedron etc.) in $C_{\epsilon_1}$ will also be present in $C_{\epsilon_2}$. Therefore, each topological feature will be "born" at some $\epsilon_{birth}$ value, and "die" at some some $\epsilon_{death}$ value. Consider the example of a loop - a loop will be "born" when the last connection around the circumference of the loop is connected (at the $\epsilon$ value which is the largest distance between consecutive points around the loop), and the loop will "die" when the last connection across the loop's diameter is connected.

Therefore, the output of persistent homology, a persistence diagram, in each dimension has one 2D point for each topological feature found in the filtration process, where the $x$-value of the point is the birth $\epsilon$ value and the $y$-value is the death $\epsilon$ value. This is why every point lies above the diagonal - features die after they are born! The difference of a points $y$ and $x$ value, $y-x$, is called the "persistence" of the corresponding topological feature. Points which have high (large) persistence likely represent real topological features of the dataset, whereas points with low persistence likely represent topological noise.

Since persistence diagrams can contain different numbers of points, and the ordering of points does not matter, there is no shared Euclidean space for all persistence diagrams. Therefore, most machine learning and statistical techniques are not directly applicable to persistence diagrams. Nevertheless, we can apply a number of these techniques to persistence diagrams provided we can quantify how near (similar) or far (distant) they are from each other, and describing suitable distance and similarity measures with their accompanying analysis methods will be the content of the following section.

# TDAML methods and underlying theory

In this section we will describe the various computational tools implemented in the package TDAML to analyze persistence diagrams, both explaining the mathematics and providing functional examples.

```{r setup}
devtools::load_all()
#library("TDAML")
```

## Distance between persistence diagrams

There are several ways to compute distances between persistence diagrams in the same dimension (where we subset only the points in the desired dimension) - the most common two are called the 2-wasserstein and bottleneck distances (@distance_calc). These techniques find an optimal matching of the 2D points in their input two diagrams, and compute a cost of that optimal matching. A point from one diagram is allowed either to be paired (matched) with a point in the other diagram or its diagonal projection, i.e. the nearest point on the diagonal line $y=x$, using either the Euclidean distance or max absolute difference in either coordinate (called the infinity-norm distance).

```{r,echo = F,out.width="80%",out.height="90%"}
D1 = data.frame(birth = c(2),death = c(3))
D2 = data.frame(birth = c(2,0),death = c(3.3,0.5))
D3 = data.frame(birth = c(0),death = c(0.5))
par(mfrow = c(2,3))
plot(x = D1$birth,y = D1$death,xlab = "Diagram 1",ylab = "",xlim = c(0,4),ylim = c(0,4))
abline(a = 0,b = 1)
plot(x = D2$birth,y = D2$death,xlab = "Diagram 2",ylab = "",xlim = c(0,4),ylim = c(0,4))
abline(a = 0,b = 1)
plot(x = c(D1$birth,D2$birth),y = c(D1$death,D2$death),xlab = "Good matching between D1 and D2 - small distance",ylab = "",xlim = c(0,4),ylim = c(0,4))
abline(a = 0,b = 1)
lines(c(2,2),c(3,3.3))
lines(c(0,0.25),c(0.5,0.25))

plot(x = D1$birth,y = D1$death,xlab = "Diagram 1",ylab = "",xlim = c(0,4),ylim = c(0,4))
abline(a = 0,b = 1)
plot(x = D3$birth,y = D3$death,xlab = "Diagram 3",ylab = "",xlim = c(0,4),ylim = c(0,4))
abline(a = 0,b = 1)
plot(x = c(D1$birth,D3$birth),y = c(D1$death,D3$death),xlab = "Poor matching between D1 and D3 - large distance",ylab = "",xlim = c(0,4),ylim = c(0,4))
abline(a = 0,b = 1)
lines(c(2,2.5),c(3,2.5))
lines(c(0,0.25),c(0.5,0.25))
par(mfrow = c(1,1))
```

Allowing points to be paired with their diagonal projections both allows for matchings of persistence diagrams with different numbers of points (which is almost always the case in practice) and also formalizes the idea that some points in a persistence diagram represent noise. The "cost" value associated with a matching is given by either (i) the maximum of infinity-norm distances between paired points, or (ii) the square-root of the sum of squared infinity-norm between matched points. The cost of the optimal matching under loss (i) is called the bottleneck distance of persistence diagrams, and the cost of the optimal matching of cost (ii) is called the 2-wasserstein metric of persistence diagrams. Both distance metrics have been used in a number of applications, but the 2-wasserstein metric is able to find more fine-scale differences in persistence diagrams compared to the bottleneck distance. The problem of finding an optimal matching can be solved with the Hungarian algorithm, which is implemented in the R package clue (@R-clue).

Another distance metric between persistence diagrams, which will be useful for kernel calculations, is called the Fisher information metric, $d_{FIM}(D_1,D_2,\sigma)$ (details can be found in @persistence_fisher). The idea is to represent the two persistence diagrams as probability density functions, with a 2D-Gaussian point mass centered at each point in both diagrams (including the diagonal projections of the points in the opposite diagram) all of variance $\sigma^2 > 0$, and calculate how much those distributions agree on their pdf value at each point in the plane (called their Fisher information metric). 

```{r,echo = F,out.width="80%",out.height="90%"}
par(mfrow = c(2,3))
D1 = data.frame(birth = c(2),death = c(3))
D2 = data.frame(birth = c(0),death = c(3))
x = seq(-4,4,0.01)
y = seq(-4,4,0.01)

plot(x = D1$birth,y = D1$death,xlab = "Diagram 1",ylab = "",xlim = c(-4,4),ylim = c(-4,4))
abline(a = 0,b = 1)
D1 = data.frame(birth = c(2,1.5),death = c(3,1.5))
z1 = outer(x,y,FUN = function(x,y){
  
  # sigma = 1
  return((exp(-((x-D1[1,1])^2+(y-D1[1,2])^2)/(2*1^2)))/sqrt(2*pi*1^2) + 
           (exp(-((x-D1[2,1])^2+(y-D1[2,2])^2)/(2*1^2)))/sqrt(2*pi*1^2))
  
})
z1 = z1/sum(z1)
image(x = x,y = y,z1,xlab = "Distribution for diagram 1",xlim = c(-4,4),ylim = c(-4,4),ylab = "")
abline(a = 0,b = 1)
image(x = x,y = y,z1,xlab = "Some points agree, others disagree",xlim = c(-4,4),ylim = c(-4,4),ylab = "")
abline(a = 0,b = 1)
points(x = c(2,0),y = c(2.5,3))

plot(x = D2$birth,y = D2$death,xlab = "Diagram 2",ylab = "",xlim = c(-4,4),ylim = c(-4,4))
abline(a = 0,b = 1)
D2 = data.frame(birth = c(0,2.5),death = c(3,2.5))
z2 = outer(x,y,FUN = function(x,y){
  
  # sigma = 1
  return((exp(-((x-D2[1,1])^2+(y-D2[1,2])^2)/(2*1^2)))/sqrt(2*pi*1^2) + (exp(-((x-D2[2,1])^2+(y-D2[2,2])^2)/(2*1^2)))/sqrt(2*pi*1^2))
  
})
z2 = z2/sum(z2)
image(x = x,y = y,z2,xlab = "Distribution for diagram 2",xlim = c(-4,4),ylim = c(-4,4),ylab = "")
abline(a = 0,b = 1)

image(x = x,y = y,z2,xlab = "Some points agree, others disagree",xlim = c(-4,4),ylim = c(-4,4),ylab = "")
abline(a = 0,b = 1)
points(x = c(2,0),y = c(2.5,3))
par(mfrow = c(1,1))
```

# Algorithmicly, we calculate the following two vectors: $\rho_1 = [\sum_{u \in D_1 \cup \Delta D_2}N(x;u,\sigma^2)|u \in D_1 \cup \Delta D_2 \cup D_2 \cup \Delta D_1]$ and $\rho_2 = [\sum_{u \in D_2 \cup \Delta D_1}N(x;u,\sigma^2)|u \in D_1 \cup \Delta D_2 \cup D_2 \cup \Delta D_1]$. Then we noramlize them by their sums, $\overline{\rho_1} = \frac{\rho_1}{\sum \rho_1}$ and $\overline{\rho_2} = \frac{\rho_2}{\sum \rho_2}$. Finally, we compute $<\sqrt{\rho_1},\sqrt{\rho_2}>$, i.e. the dot product of the elementwise square root of both of the normalized vectors.

These distance calculations between persistence diagrams have been implemented in the `diagram_distance` function, providing a fast native-R computation for all three distance metrics which can be used for standalone analyses or machine learning/inference in other TDAML functions. Note that there is a generalization of the 2-wasserstein distance for any $p \geq 1$, the $p$-wasserstein distance, which can also be computed using the `diagram_distance` function. Here is an example of computing distances between a persistence diagram sampled from a sphere and from a torus:

```{r,echo = T}
# create two diagrams with package TDA based on torus and sphere
torus <- TDA::ripsDiag(TDA::torusUnif(n = 100,a = 1,c = 2),
maxscale = 2,
maxdimension = 2)
sphere <- TDA::ripsDiag(TDA::sphereUnif(n = 100,d = 2,r = 1),
maxscale = 2,
maxdimension = 2)

# calculate their wasserstein distance in dimension 1
diagram_distance(D1 = torus,D2 = sphere,dim = 1,p = 2,distance = "wasserstein")

# calculate their bottleneck distance in dimension 2
diagram_distance(D1 = torus,D2 = sphere,dim = 2,p = Inf,distance = "wasserstein")

# Fisher information metric calculation in dimension 1
diagram_distance(D1 = torus,D2 = sphere,dim = 1,distance = "fisher",sigma = 1)

# Fisher information metric calculation in dimension 2
diagram_distance(D1 = torus,D2 = sphere,dim = 2,distance = "fisher",sigma = 1)
```

As we can see, the torus and sphere are more similar in dimension 2 than they are in dimension 1, since both contain one void but the torus contains two loops and the sphere contains none.

## Metric multidimensional scaling of persistence diagrams

One common dimension-reduction technique in machine learning is called multidimensional scaling (MDS) (@Cox2008). MDS takes as input an $n$ by $n$ distance (or dissimilarity) matrix $D$, computed from $n$ points in a dataset, and outputs an embedding of those points into a Euclidean space of chosen dimension $k$ which best preserves the inter-point distances. MDS is often used for visualizing data in exploratory analyses, and can be particularly useful when the input data points do not live in a common Euclidean space (as is the case for persistence diagrams). Using the R function `cmdscale` from the package stats (@R-stats) we can compute the optimal embedding of a set of persistence diagrams using any of the three distance metrics using the function `diagram_MDS`. Here is an example of the `diagram_MDS` function on nine persistence diagrams, three sampled from each of a circle, sphere and a torus:

```{r,echo = T,out.width="80%",out.height="90%"}
# create 9 diagrams with package TDA based on spheres, circles and tori
g <- lapply(X = 1:9,FUN = function(X){

  if(X %% 3 == 0)
  {
    df <- TDA::ripsDiag(TDA::circleUnif(n = 100,r = 1),maxdimension = 1,maxscale = 2)
  }
  if(X %% 3 == 1)
  {
    df <- TDA::ripsDiag(TDA::sphereUnif(n = 100,d = 2,r = 1),maxdimension = 1,maxscale = 2)
  }
  if(X %% 3 == 2)
  {
    df <- TDA::ripsDiag(TDA::torusUnif(n = 100,a = 1,c = 2),maxdimension = 1,maxscale = 2)
  }
  df <- diagram_to_df(d = df)
  return(df)

})

# calculate their 2D MDS embedding in dimension 1 with the bottleneck distance
mds <- diagram_MDS(diagrams = g,dim = 1,p = Inf,k = 2)

# plot
par(mar=c(5.1, 4.1, 4.1, 8.1), xpd=TRUE)
plot(mds[,1],mds[,2],xlab = "Embedding dimension 1",ylab = "Embedding dimension 2",col = as.factor(rep(c("circle","sphere","torus"),3)),bty = "L")
legend("topright", inset=c(-0.2,0), legend=levels(as.factor(c("circle","sphere","torus"))), pch=16, col=unique(as.factor(c("circle","sphere","torus"))))
```

The MDS plot shows the clear separation between the three groups, and the embedded coordinates could be used for further downstream analyses.

## Testing for group differences of persistence diagrams

Distributions of persistence diagrams can be complicated (see @PD_means and @PD_distributions). Therefore, a non-parametric permutation test has been proposed which can find differences in groups of persistence diagrams. Such a test was first proposed in @Robinson_Turner, and some variations have been suggested in later publications. In @Robinson_Turner, two groups of persistence diagrams would be compared. The null hypothesis, $H_0$, is that the diagrams from the two groups are generated from shapes with the same type and scale of topological features. The alternative hypothesis, $H_A$, is that the underlying type or scale of the features are different between the two groups. In each dimension a p-value is computed, finding evidence against $H_0$ in that dimension. A measure of within-group distances (a "loss function") is calculated for the two groups, and that measure is compared to a null distribution for when the group labels are permuted.

This inference procedure is implemented in the `permutation_test` function, with several speedups and additional functionalities. Firstly, the loss function is computed in parallel for scalability since distance computations can be expensive. Secondly, we store distance calculations as we compute them because these calculations are often repeated. Additional functionality includes allowing for any number groups (not just two) and allowing for a pairing between groups of the same size as described in (@dependencies). When a natural pairing exists between the groups (like if the groups represent persistence diagrams from the same subject of a study in different conditions) we can simulate a more realistic null distribution by restricting the way in which we permute group labels, achieving higher statistical power. 

In order to demonstrate the utility of the permutation test we will create two groups of diagrams, coming from small random deviations from the following two diagrams:

```{r,echo = F,out.width="80%",out.height="90%"}
D1 <- data.frame(dimension = as.factor(c(0,1)),birth = c(0,1),death = c(2,2))
D2 <- data.frame(dimension = as.factor(c(0,1)),birth = c(0,0.1),death = c(2,2))
par(mfrow = c(1,3),mar = c(1,1,1,1))
par(fig = c(0,4,0,10)/10)
plot(D1$birth,D1$death,xlab = "Birth",ylab = "Death",col = D1$dimension,xlim = c(0,2.5),ylim = c(0,2.5),main = "Diagram 1")
abline(a = 0,b = 1)
par(fig=c(4,8,0,10)/10)
par(new=T)
plot(D2$birth,D2$death,xlab = "Birth",ylab = "Death",col = D2$dimension,xlim = c(0,2.5),ylim = c(0,2.5),main = "Diagram 2")
abline(a = 0,b = 1)
par(fig=c(8,10,0,10)/10)
par(new=T)
plot(1, type = "n", axes=FALSE, xlab="", ylab="")
legend(x = "center",inset = 0,legend = c("0","1"),col = c("black","red"),pch = 1,cex = 2)
```

There should be no difference between the groups in dimension 0, and a difference between the groups in dimension 1:

```{r,echo = T}
# create two groups of persistence diagrams from Diagram 1 and Diagram 2 respectively
g1 <- lapply(X = 1:10,FUN = function(X){
  
  t <- D1
  t$dimension <- as.numeric(as.character(t$dimension))
  t$birth <- t$birth + stats::rnorm(n = 2,mean = 0,sd = 0.05)
  t[which(t$birth < 0),2] <- 0
  t$death <- t$death + stats::rnorm(n = 2,mean = 0,sd = 0.05)
  return(t)
  
})

g2 <- lapply(X = 1:10,FUN = function(X){
  
  t <- D2
  t$dimension <- as.numeric(as.character(t$dimension))
  t$birth <- t$birth + stats::rnorm(n = 2,mean = 0,sd = 0.05)
  t[which(t$birth < 0),2] <- 0
  t$death <- t$death + stats::rnorm(n = 2,mean = 0,sd = 0.05)
  return(t)
  
})

# do permutation test with 30 iterations, p,q = 2, in dimensions 0 and 1, with
# no pairing using wasserstein distance 
permutation_test(g1,g2,
                 iterations = 30,
                 dims = c(0,1))$p_values
```

As expected, no difference was found between the groups in dimension 0 but a difference was found (at the $\alpha = 0.05$ significance level) between the groups in dimension 1.

## Kernels between persistence diagrams

A kernel function is a special (positive semi-definite) symmetric similarity measure between objects in some complicated space (see (@murphyML) for a good discussion of kernel methods). Kernels can be used to map complex objects into a high-dimensional space amenable to machine learning and other tasks - some examples of machine learning techniques which can be "kernelized" when dealing with complicated data are k-means (kernel k-means), principal components analysis (kernel PCA), and support vector machines (which are inherently based on kernel calculations).

There have been, to date, four main kernels proposed for persistence diagrams. In TDAML the persistence Fisher kernel (@persistence_fisher) has been implemented because of its practical advantages over the other kernels - smaller cross-validation SVM error on a number of test data sets and a faster method for cross validation. However, for information on the other three kernels see (@kernel_TDA), (@sliced_wasserstein), and (@kernel_TDA_original).

The persistence Fisher kernel is computed directly from the Fisher information metric between two persistence diagrams: let $\sigma > 0$ be the parameter for $d_{FIM}$, and let $t > 0$. Then the persistence Fisher kernel is defined as $k_{PF}(D_1,D_2) = \mbox{exp}(-t*d_{FIM}(D_1,D_2,\sigma))$. Computing the persistence Fisher kernel can be achieved with the `diagram_kernel` function in TDAML - here is an example for a torus and sphere:

```{r,echo = T}
# create two diagrams with package TDA based on torus and sphere
torus <- TDA::ripsDiag(TDA::torusUnif(n = 100,a = 1,c = 2),
maxscale = 2,
maxdimension = 2)
sphere <- TDA::ripsDiag(TDA::sphereUnif(n = 100,d = 2,r = 1),
maxscale = 2,
maxdimension = 2)

# calculate their kernel value in dimension 2 with sigma = 2, t = 2
diagram_kernel(D1 = torus,D2 = sphere,dim = 2,sigma = 2,t = 2)
```

## Kernel k-means of persistence diagrams

Kernel k-means (@kkmeans) is a method which extends regular k-means clustering via a kernel on complex data. The "distance" from a persistence diagram to a cluster center is calculated using kernels, and the algorithm converges like regular k-means. This algorithm is implemented in the function `diagram_kkmeans` as a wrapper of the kernlab function `kkmeans`. Moreover, a prediction function `diagram_nearest_clusters` can be used to find the nearest cluster labels for a new set of diagrams. Here is an example clustering two groups of different diagrams:

```{r,echo = T}
# concatenate the two lists based on diagram 1 and diagram 2, they should be clearly separable in dimension 1
g <- append(g1,g2)

# calculate kmeans clusters with centers = 2 in dimension 1 with sigma = t = 2
clust <- diagram_kkmeans(diagrams = g,centers = 2,dim = 1,t = 2,sigma = 2)

# display cluster labels
clust$clustering@.Data

# display cluster within-cluster sum of squared distances
clust$clustering@withinss
```

As we can see, the `diagram_kkmeans` function was able to correctly separate the two original groups of diagrams.

If we wish to predict the cluster label for new persistence diagrams (computed via the largest kernel value to any cluster center), we can use the `diagram_nearest_clusters` function as follows:

```{r,echo = T}
# create ten new diagrams with package TDA based on spheres and circles
g_new <- lapply(X = 1:10,FUN = function(X){
  
  if(X <= 5)
  {
    t <- D1
    t$dimension <- as.numeric(as.character(t$dimension))
    t$birth <- t$birth + stats::rnorm(n = 2,mean = 0,sd = 0.05)
    t[which(t$birth < 0),2] <- 0
    t$death <- t$death + stats::rnorm(n = 2,mean = 0,sd = 0.05)
  }else
  {
    t <- D2
    t$dimension <- as.numeric(as.character(t$dimension))
    t$birth <- t$birth + stats::rnorm(n = 2,mean = 0,sd = 0.05)
    t[which(t$birth < 0),2] <- 0
    t$death <- t$death + stats::rnorm(n = 2,mean = 0,sd = 0.05)
  }

  return(t)

})

# predict cluster labels
diagram_nearest_clusters(new_diagrams = g_new,clustering = clust)
```

This function correctly predicted the cluster for each new diagram (assigning each diagram to the cluster label by Diagram 1 or Diagram 2, depending on which diagram it was generated from).

## Kernel principal components analysis of persistence diagrams

Kernel PCA (@kpca) is an extension of regular PCA which uses a kernel to project complex data into a high-dimensional Euclidean space and then uses PCA to project that data into a low-dimensional space. The `diagram_kpca` method computes the kPCA embedding of a set of persistence diagrams, and the `predict_diagram_kpca` function can be used to project new diagrams using a pretrained kPCA model. Here is an example using a group of persistence diagrams from spheres and circles:

```{r,echo = T,out.width="80%",out.height="90%"}
# create ten diagrams with package TDA based on spheres and circles
g <- lapply(X = 1:10,FUN = function(X){
  
  if(X <= 5)
  {
    diag <- TDA::ripsDiag(TDA::circleUnif(n = 100,r = 1),maxscale = 2,maxdimension = 1)
  }else
  {
    diag <- TDA::ripsDiag(TDA::sphereUnif(n = 100,d = 2,r = 1),maxscale = 2,maxdimension = 1)
  }
  
  df <- diagram_to_df(d = diag)
  return(df)

})

# calculate their 2D PCA embedding in dimension 1 with sigma = t = 2
pca <- diagram_kpca(diagrams = g,dim = 1,t = 2,sigma = 2,features = 2)

# plot
par(mar=c(5.1, 4.1, 4.1, 8.1), xpd=TRUE)
plot(pca$pca@rotated[,1],pca$pca@rotated[,2],xlab = "Embedding dimension 1",ylab = "Embedding dimension 2",col = as.factor(rep(c("circle","sphere"),each = 5)))
legend("topright",inset = c(-0.2,0), legend=levels(as.factor(c("circle","sphere"))), pch=16, col=unique(as.factor(c("circle","sphere"))))

```

In dimension 1 the circle diagrams are all very similar, and this is detected by the `diagram_kpca` function, whereas there is more variability in the dimension 1 structure when sampling a sphere so the sphere points are more spread out.

We can use the `predict_diagram_kpca` function to project new persistence diagrams into low dimensions using a predefined PCA embedding as follows:

```{r,echo = T,out.width="80%",out.height="90%"}
# create ten new diagrams with package TDA based on spheres and circles
g_new <- lapply(X = 1:10,FUN = function(X){
  
  if(X <= 5)
  {
    diag <- TDA::ripsDiag(TDA::circleUnif(n = 100,r = 1),maxscale = 2,maxdimension = 1)
  }else
  {
    diag <- TDA::ripsDiag(TDA::sphereUnif(n = 100,d = 2,r = 1),maxscale = 2,maxdimension = 1)
  }
  
  df <- diagram_to_df(d = diag)
  return(df)

})

# project new diagrams onto old model
new_pca <- predict_diagram_kpca(new_diagrams = g_new,embedding = pca)

# plot
par(mar=c(5.1, 4.1, 4.1, 8.1), xpd=TRUE)
plot(new_pca[,1],new_pca[,2],xlab = "Embedding dimension 1",ylab = "Embedding dimension 2",col = as.factor(rep(c("circle","sphere"),each = 5)))
legend("topright",inset = c(-0.2,0), legend=levels(as.factor(c("circle","sphere"))), pch=16, col=unique(as.factor(c("circle","sphere"))))
```
Once again the circle points are tightly packed together, whereas the sphere points are not, and the new circle points are close in location to the location of the original circle points.

## Kernel support-vector machines of persistence diagrams

Support vector machines (SVM) (@murphyML) are one of the most popular machine learning techniques for regression and classification tasks. SVMs use a kernel function to project complex data into a high-dimensional space and then find a sparse set of training examples, called "support vectors", which maximally linearly separate the outcome variable classes (or yield the highest explained variance in the case of regression).

SVMs have been implemented in the function `diagram_ksvm` for when the input data set consists of a single feature which is a persistence diagram and an outcome-variable vector. A prediction method is supplied called `predict_diagram_ksvm` which can be used to predict the label value of a set of new persistence diagrams given a pretrained model. A parallelized implementation of cross-validation model-fitting is used based on the remarks in (@persistence_fisher) for scalability (which avoids needlessly recomputing persistence Fisher information metric values). Here is an example of fitting an SVM model on a list of persistence diagrams for a classification task (guessing whether the diagram comes from a sphere or a circle):

```{r,echo = T}
# create thirty diagrams with package TDA based on tori and circles
g <- lapply(X = 1:30,FUN = function(X){
  
  if(X <= 15)
  {
    diag <- TDA::ripsDiag(TDA::torusUnif(n = 100,a = 1,c = 2),maxscale = 2,maxdimension = 2)
  }else
  {
    diag <- TDA::ripsDiag(TDA::circleUnif(n = 100,r = 1),maxscale = 2,maxdimension = 2)
  }
  
  df <- diagram_to_df(d = diag)
  return(df)

})

# create response vector
y <- as.factor(c(rep("torus",15),rep("circle",15)))

# fit model with cross validation
model_svm <- diagram_ksvm(diagrams = g,cv = 2,dim = c(1,2),y = y,sigma = c(1,0.1))
```

We can use the function `predict_diagram_ksvm` to predict new diagrams like so:

```{r,echo = T}
# create ten new diagrams with package TDA based on spheres and circles
g_new <- lapply(X = 1:10,FUN = function(X){
  
  if(X <= 5)
  {
    diag <- TDA::ripsDiag(TDA::circleUnif(n = 100,r = 1),maxscale = 2,maxdimension = 2)
  }else
  {
    diag <- TDA::ripsDiag(TDA::torusUnif(n = 100,a = 1,c = 2),maxscale = 2,maxdimension = 2)
  }
  
  df <- diagram_to_df(d = diag)
  return(df)

})

# answer should be c(rep("circle",5),rep("torus",5))
predict_diagram_ksvm(new_diagrams = g_new,model = model_svm)
```

As we can see the best SVM model was able to separate the two classes. We can gain more information about the best model found during model fitting and the CV results by accessing different list elements of `model_svm`.

## Testing for independence between two groups of paired persistence diagrams

An important question when presented with two groups of paired persistence diagrams is determining if the pairings are independent or not. A procedure was described in @kernel_test which can be used to answer this question using kernel computations, and importantly uses a parametric null distribution. A test statistic called the Hilbert-Schmidt independence criteria is calculated, and its value is compared to a gamma distribution with certain parameters which can be estimated from the data.

#: let $(D_{i},D'_{i})$ be a collection of $m$ paired samples of persistence diagrams. We will compute an estimated measure of independence (which would be calculated as correlation using the regular Euclidean kernel dot product for vectors) called the Hilbert-Schmidt independence criteria (HSIC), but to do so we need to define a few matrices based on the kernel calculations between the persistence diagrams in our sample. Let $K$ be the matrix of all pairwise persistence Fisher kernel values between all the $D_i$'s, i.e. $K[i,j] = k_{PF}(D_i,D'_i)$, and $L$ be the same for the $D'_i$'s. Finally, let $H$ be the matrix in dimension $m$ (with $m$ rows and columns) which has value 0 on the diagonal and -1 everywhere else, and define $m_n = \frac{m!}{(m-n)!}$. Then we calculate HSIC as $\frac{1}{m^2}\mbox{trace}(K*H*L*H)$, where the trace operation is the sum of the diagonal entries of the result of the matrix multiplication. It turns out that under the null hypothesis of independence that the distribution of HSIC is almost perfectly estimated by a Gamma distribution. The mean of the distribution is computed as

# $\frac{1}{m}(1+||\mu_{x}||^2*||\mu_{y}||^2-||\mu_x||^2-||\mu_y||^2)$, 

# where $||\mu_x||^2$ is approximately the mean of kernel values between the $x_i$'s (excluding the kernel values of each $x_i$ with itself, which is always 1), and likewise for $||\mu_y||^2$. The variance of HSIC can be approximated by

# $\frac{2(m-4)(m-5)}{m_4}1(B-diag(B))1^T$, 

# where $1$ is the vector of length $m$ consisting of all 1's and $B$ is the matrix computed as the elementwise squared matrix of the Hadamard (element-wise) product of the matrices $HKH$ and $HLH$, where $H$, $K$ and $L$ are as before.

This inference procedure has been implemented in the `independence_test` function, and returns the p-value of the test in each desired dimension of the diagrams (among other additional information). Here is an example comparing two groups of independent diagrams sampled from circles and spheres:

```{r,echo = T}
# create two groups of persistence diagrams from circles and spheres using TDA
circles <- lapply(X = 1:10,FUN = function(X){

  diag <- TDA::ripsDiag(TDA::circleUnif(n = 100,r = 1),
  maxscale = 2,
  maxdimension = 1)
  df <- diagram_to_df(d = diag)
  return(df)

})

spheres <- lapply(X = 1:10,FUN = function(X){

  diag <- TDA::ripsDiag(TDA::sphereUnif(n = 100,d = 2,r = 1),
  maxscale = 2,
  maxdimension = 1)
  df <- diagram_to_df(d = diag)
  return(df)

})

# do independence test with sigma = 1, t = 1, in dimensions 0 and 1, printing the time duration
independence_test(circles,spheres,verbose = TRUE)$p_value
```

The p-values of this test (in dimensions 0 and 1) would not be significant at any typical significance threshold, reflecting the fact that there is no real (i.e. non-spurious) dependence between the two groups.

# Case study: `Benchmarking the diagram_distance function`

Computing (wasserstein/bottleneck) distances between persistence diagrams is a key feature of some of the main topological data analysis software packages. However, these calculations can be very expensive, rendering practical applications of topological data analysis nearly unfeasible. TDAML strives to provide useful methods for applied topological data analysis, and as such its distance function must be fast, at least compared to distance calculations provided by other packages. We will compare the run time of the TDAML `diagram_distance` function compared to the R package TDA `wasserstein` function to argue that TDAML provides a much more tractable distance calculation function. Note that the python package scikit-TDA also implements distance and kernel calculations, however in order to benchmark against scikit-TDA for the purpose of this package vignette we would have to introduce dependencies on the reticulate package @R-reticulate, and since reticulate is known to slow down python performance it would even be unclear how to interpret positive results.

We will first simulate pairs of random diagrams of various sizes ($100,200,300,\dots,10000$), with 10 iterations at each size to compare run times. Each diagram has birth values of 0.1 multiplied by a random sample of a chi squared distribution with 10 degrees of freedom, and the death was the birth plus 0.1 times a sample from a chi squared distribution with 1 degree of freedom. The distance between each pair of diagrams will be computed using the TDA `wasserstein` function and the TDAML `diagram_distance` function with paramters set for the wasserstein distance and $p = 2$, and the run time for each calculation was recorded on a Windows 10 64-bit machine, an Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz 3.60 GHz processor with 8 cores and 64GB of RAM. The code for benchmarking was as follows, however the code is not run in this vignette since it takes too long:

```{r,echo = T,eval = F}
# simulate random barcodes with sizes 100,200,...,1000
runtimes_random <- data.frame(n_row = numeric(),package = character(),time_in_sec = numeric())
for(n_row in seq(100,1000,100)){

  for(iteration in 1:10)
  {

    # simulate pair of diagrams, points mainly close to the diagonal
    diagram1 = data.frame(dimension = rep(0,n_row),birth = 0.1*rchisq(n = n_row,df = 10))
    diagram1$death = diagram1$birth + 0.1*rchisq(n = n_row,df = 1)
    diagram2 = data.frame(dimension = rep(0,n_row),birth = 0.1*rchisq(n = n_row,df = 10))
    diagram2$death = diagram2$birth + 0.1*rchisq(n = n_row,df = 1)

    # benchmark time, in seconds, for both package's distance calculations (2-wasserstein distance in TDAML)
    start_time_TDAML = Sys.time()
    d_TDAML = diagram_distance(D1 = diagram1,D2 = diagram2,dim = 0,p = 2,distance = "wasserstein")
    end_time_TDAML = Sys.time()
    time_diff_TDAML = as.numeric(end_time_TDAML - start_time_TDAML,units = "secs")
    diagram1 = list(diagram = diagram1)
    diagram2 = list(diagram = diagram2)
    diagram1$diagram = as.matrix(diagram1$diagram)
    diagram2$diagram = as.matrix(diagram2$diagram)
    class(diagram1$diagram) = "diagram"
    class(diagram2$diagram) = "diagram"
    start_time_TDA = Sys.time()
    d_TDA = TDA::wasserstein(Diag1 = diagram1$diagram,Diag2 = diagram2$diagram,dimension = 0,p = 2)
    end_time_TDA = Sys.time()
    time_diff_TDA = as.numeric(end_time_TDA - start_time_TDA,units = "secs")

    runtimes_random = rbind(runtimes_random,data.frame(n_row = n_row,package = "TDAML",time_in_sec = time_diff_TDAML))
    runtimes_random = rbind(runtimes_random,data.frame(n_row = n_row,package = "TDA",time_in_sec = time_diff_TDA))

  }

}

# compute means and sd's at each value of rows for both packages
summary_table = data.frame(n_row = numeric(),mean = numeric(),sd = numeric(),package = character())
for(n_row in seq(100,1000,100))
{

  for(p in c("TDAML","TDA"))
  {

    summary_table = rbind(summary_table,data.frame(n_row = n_row,mean = mean(runtimes_random[which(runtimes_random$n_row == n_row & runtimes_random$package == p),3]),sd = sd(runtimes_random[which(runtimes_random$n_row == n_row & runtimes_random$package == p),3]),package = p))

  }

}

```

The results were as follows:

```{r,echo = F,warning=F,out.width="80%",out.height="90%"}
summary_table = data.frame(n_row = rep(seq(100,1000,100),each = 2),mean = c(0.087,0.137,0.278,1.880,0.859,8.742,1.782,28.908,3.928,63.544,6.648,141.373,11.158,252.689,18.409,427.515,28.220,725.520,39.675,1024.007),sd = c(0.021,0.010,0.018,0.170,0.100,0.630,0.161,1.849,0.245,3.927,0.455,12.202,0.806,14.315,1.350,37.318,2.675,63.963,2.983,102.328),package = rep(c("TDAML","TDA"),10))

# plot table
plot(summary_table$n_row[summary_table$package=="TDA"], summary_table$mean[summary_table$package=="TDA"], type="b",
     xlim=range(summary_table$n_row), ylim=range(0,summary_table$mean+1.96*summary_table$sd),xlab = "Points in diagram",ylab = "Mean execution time (sec)",main = "Benchmarking diagram_distance function")
lines(summary_table$n_row[summary_table$package=="TDAML"], summary_table$mean[summary_table$package=="TDAML"], col=2, type="b")
legend(x = 200,y = 800,legend = c("TDAML","TDA"),col = c("red","black"),lty = c(1,1),cex = 0.8)
arrows(summary_table$n_row, summary_table$mean-1.96*summary_table$sd/sqrt(10), summary_table$n_row, summary_table$mean+1.96*summary_table$sd/sqrt(10), length=0.05, angle=90, code=3)
```

The TDAML `diagram_distance` function gives a significant speedup compared to the TDA wasserstein function (with similar results expected for the `bottleneck` distance function since the two use the same underlying matching method). As the diagrams get larger the speedup appears to increase, with almost a 26x speedup for diagrams with 1000 points, however this ratio looks like it would plateau around 30x above 1000 points, or at least it would grow slowly. 

Now, we will repeat the same simulation, but computing diagrams from shapes provided in the TDA package (uniformly sampled tori and spheres) with the same number of data points and iterations as in the previous example. The homology calculations had a maximum dimension of two, and the maximum scale for each was 2 (for the Torus) and 1 (for the sphere). The Torus had tube radius 1 and cross-radius 2. Again, to avoid a long build for this vignette the results were computed and recorded, but the code used for benchmarking can be seen below (not run in this vignette):

```{r,echo = F,warning=F,eval = F}
# generate persistence diagrams from Tori and spheres with  100,200,...,1000 data points.
runtimes_shape <- data.frame(n_row = numeric(),package = character(),time_in_sec = numeric())
for(n_row in seq(100,1000,100)){

  for(iteration in 1:10)
  {
    # simulate pair of diagrams from the desired shapes
    diagram_torus = ripsDiag(X = TDA::torusUnif(n = n_row,a = 1,c = 2),maxdimension = 2,maxscale = 2)
    diagram_sphere = ripsDiag(X = TDA::sphereUnif(n = n_row,d = 2,r = 1),maxdimension = 2,maxscale = 1)

    # compute their wasserstein distances in all dimensions and benchmark
    start_time_TDAML = Sys.time()
    diagram_distance(D1 = diagram_torus,D2 = diagram_sphere,dim = 0,p = 2,distance = "wasserstein")
    diagram_distance(D1 = diagram_torus,D2 = diagram_sphere,dim = 1,p = 2,distance = "wasserstein")
    diagram_distance(D1 = diagram_torus,D2 = diagram_sphere,dim = 2,p = 2,distance = "wasserstein")
    end_time_TDAML = Sys.time()
    time_diff_TDAML = as.numeric(end_time_TDAML - start_time_TDAML,units = "secs")

    start_time_TDA = Sys.time()
    TDA::wasserstein(Diag1 = diagram_torus$diagram,Diag2 = diagram_sphere$diagram,dimension = 0,p = 2)
    TDA::wasserstein(Diag1 = diagram_torus$diagram,Diag2 = diagram_sphere$diagram,dimension = 1,p = 2)
    TDA::wasserstein(Diag1 = diagram_torus$diagram,Diag2 = diagram_sphere$diagram,dimension = 2,p = 2)
    end_time_TDA = Sys.time()
    time_diff_TDA = as.numeric(end_time_TDA - start_time_TDA,units = "secs")

    runtimes_shape = rbind(runtimes_shape,data.frame(n_row = n_row,package = "TDAML",time_in_sec = time_diff_TDAML))
    runtimes_shape = rbind(runtimes_shape,data.frame(n_row = n_row,package = "TDA",time_in_sec = time_diff_TDA))

  }
  print(paste0("Done ",n_row," rows"))

}

# compute means and sd's at each value of rows for both packages
summary_table = data.frame(n_row = numeric(),mean = numeric(),sd = numeric(),package = character())
for(n_row in seq(100,1000,100))
{
  for(p in c("TDAML","TDA"))
  {
    summary_table = rbind(summary_table,data.frame(n_row = n_row,mean = mean(runtimes_shape[which(runtimes_shape$n_row == n_row & runtimes_shape$package == p),3]),sd = sd(runtimes_shape[which(runtimes_shape$n_row == n_row & runtimes_shape$package == p),3]),package = p))
  }
}

```

The results again showed a significant performance increase in the TDAML package:

```{r,echo = F,warning=F,out.width="80%",out.height="90%"}
summary_table = data.frame(n_row = rep(seq(100,1000,100),each = 2),mean = c(0.133,0.220,0.398,3.430,0.953,17.054,1.955,53.896,3.760,142.524,6.604,294.552,10.466,601.802,16.169,1130.289,23.458,2091.953,35.098,3518.517),sd = c(0.022,0.016,0.035,0.186,0.094,1.1889,0.130,4.501,0.211,5.924,0.772,11.244,0.623,9.064,0.989,43.385,1.989,88.432,2.747,172.684),package = rep(c("TDAML","TDA"),10))

# plot table
plot(summary_table$n_row[summary_table$package=="TDA"], summary_table$mean[summary_table$package=="TDA"], type="b",
     xlim=range(summary_table$n_row), ylim=range(0,summary_table$mean+1.96*summary_table$sd),xlab = "Points in shapes",ylab = "Mean execution time (sec)",main = "Benchmarking diagram_distance function on Torus and Sphere")
lines(summary_table$n_row[summary_table$package=="TDAML"], summary_table$mean[summary_table$package=="TDAML"], col=2, type="b")
legend(x = 200,y = 2000,legend = c("TDAML","TDA"),col = c("red","black"),lty = c(1,1),cex = 0.8)
arrows(summary_table$n_row, summary_table$mean-1.96*summary_table$sd/sqrt(10), summary_table$n_row, summary_table$mean+1.96*summary_table$sd/sqrt(10), length=0.05, angle=90, code=3)
```

The ratio of TDA to TDAML run time grew at each subsequent number of tested data points in the shapes, with a 100x speed up for 1000 data points. The fast distance calculation in TDAML makes the applications of statistics and machine learning with persistence diagrams more feasible, hence why the TDA distance calculation was not used in the TDAML package.

# Conclusions

The TDAML package aims to bridge topological data analysis with researchers and data practioners in the R community. Current topological data analysis packages in R and python do not provide the ability to carry out standard types of data analysis, being statistics and machine learning, with persistence diagrams - greatly limiting research and industry interest in topological data analysis. TDAML was built with performance in mind, with fast native-R implementations of distance calculations between persistence diagrams and parallelization at every possible point. Topological data analysis is an exciting and powerful new field of data analysis, and with TDAML anyone can access its power for meaningful and creative analyses of data.

## References


